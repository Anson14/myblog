#+TITLE:       InfoCom 2019 学习记录
#+AUTHOR:      tang
#+EMAIL:       tang@tangdeMacBook-Pro.local
#+DATE:        2020-03-13 Fri
#+URI:         /blog/%y/%m/%d/infocom-2019-学习记录
#+KEYWORDS:    Infocomm, Study, Network
#+TAGS:        Study
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: The study note for infocomm 2019
* A Flexible Distributed Optimization Framework for Service of Concurrent Tasks in Processing Networks

** 介绍
分布式优化：让一群没有服务器指挥的节点，在仅和邻居通讯（类似于 P2P）的情况下达到整体最优解。需要分布式优化的原因自然是因为 CS 架构的优化模型存在缺点，比如单点故障等原因，参考知乎[[https://www.zhihu.com/question/59260302][刘家耿的回答]]，给出一个简单的形式化的表达式：
$$
  \min_{x_i,\cdots,x_n} \sum_{i=1}^n f_i(x_i)
$$

上面的公示与常规的优化公示相对应，CS 架构的的优化公示中的 x 是全局统一的，存储在中心 server 上，而分布式优化的 x 则存储在每一个 client 上，这里再借用下上面的回答中的一个流程进一步的阐述分布式优化：
#+begin_src english
for each node i, simultaneously
*repeat*
    do gradient updates with own data;
    regularly exchange some information with neighbors;
    combine information according to some policy;
*until* reach consensus and optimaticaly
#+end_src

文章里描述的是，可以为机器学习和信号处理提供使用内部网络连接的处理通过间歇性通信来完成全局最有目标的方法。目前的很多分布式化算法假设所有的处理器都存储了相关数据，但实际上是很多全局最优任务是运行在共享资源的主机上的，所以需要提出一种能够灵活的和其他任务共享资源的分布式优化算法。

Here is a summary of classic and novel *Distributed Optimization* 
#+begin_quote
One of the pioneering works on distributed optimization was Tsitsiklis et al.’s work [22]. Since then, several types of methods have been proposed in this area, such as distributed subgradient descent (DSD) [8], [14], distributed dual averaging [4], [21], Alternating Direction Method of Multipliers (ADMM) [9], [18], Nesterov’s method [15], [17] and second-order algorithm [10], [23], with different performances and restrictions. Among these types, DSD is the most important algorithm because it is easily implemented in a distributed way (ADMM needs sequential variable updates and second order methods need costly distributed Hessian calculation), and the basis of many further developed algorithms. For example, by adding history gradient information to DSD, the methods in [13] and [16] can achieve a linear convergence rate for the sum of strongly convex and smooth functions with a constant stepsize. Nesterov’s method can also be considered as a variant of the gradient method. So in this paper, we will focus on gradient-based algorithms.
#+end_quote

文章提出一套灵活的分布式次梯度(subgradient)算法，允许处理器通过基于概率地在多个正在进行的任务之间切换，从而同时处理多个正在进行的任务。
#+begin_quote
当函数不可导时，无法通过常规的方法求梯度，也就无法使用梯度下降发来进行实现优化算法，因此针对存在不可导（不光滑）的函数提出其次梯度的概念，其标准定义为：

g 是函数 f 在 x 点的次梯度，当
$$
    f(y) \geq f(x) + g^T(y-x), \forall y
$$

上面的 g 是一个可能的次梯度，函数在某一点的次梯度是一个集合被记为\partial f(x)，对于一个凸函数而言，其 \partial f(x) 非空。所以可以使用类似于经典梯度下降的方法使用次梯度对不可导函数进行优化。
#+end_quote

     
* Footnotes

